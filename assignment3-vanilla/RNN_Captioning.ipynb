{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这会将您的Google驱动器装载到Colab虚拟机.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: 在保存解压文件的驱动器中输入文件夹名\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# 现在我们已经安装了你的驱动器，这确保了\n",
    "# Colab VM的Python解释器可以加载\n",
    "# python文件。\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "\n",
    "# 这会将COCO数据集下载到您的驱动器\n",
    "# 如果它还不存在的话。\n",
    "\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with RNNs\n",
    "\n",
    "在本练习中，您将实现vanilla递归神经网络，并使用它们来训练能够为图像生成新颖字幕的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell.\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.rnn_layers import *\n",
    "from cs231n.captioning_solver import CaptioningSolver\n",
    "from cs231n.classifiers.rnn import CaptioningRNN\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# COCO Dataset\n",
    "在本练习中，我们将使用2014年发布的[COCO数据集](https://cocodataset.org/)，一个标准的图像字幕测试平台。这个数据集由80000个训练图像和40000个验证图像组成，每个图像都有5个由Amazon Mechanical Turk上的工作人员编写的标题注释。\n",
    "\n",
    "**Image features.** 我们已经为您预处理了数据并提取了特征。对于所有的图像，我们都从预先在ImageNet上训练的VGG-16网络fc7层提取特征，并将这些特征存储在文件`train2014_vgg16_fc7.h5`和`val2014_vgg16_fc7.h5`中。为了减少处理时间和内存需求，我们使用主成分分析（PCA）将特征的维数从4096降到512，并将这些特征存储在文件 `train2014_vgg16_fc7_pca.h5` 和 `val2014_vgg16_fc7_pca.h5`中。原始图像占用了近20GB的空间，因此我们没有将其包含在下载中。由于所有图像都是从Flickr获取的，因此我们将训练和验证图像的url存储在文件`train2014_urls.txt` 和 `val2014_urls.txt`中。这允许您动态下载图像以进行可视化。\n",
    "\n",
    "**Captions.** 处理字符串效率很低，因此我们将使用标题的编码版本。每个单词都分配了一个整数ID，允许我们用一系列整数来表示标题。整数id和单词之间的映射在文件`coco2014_vocab.json`中，您可以使用文件`cs231n/coco_utils.py`中的函数`decode_captions`将整数id的NumPy数组转换回字符串。\n",
    "\n",
    "**Tokens.** 我们在词汇表中添加了几个特殊标记，我们已经为您处理了有关特殊标记的所有实现细节。我们在每个标题的开头和结尾分别添加一个特殊的`<START>`标记和一个`<END>`标记。罕见的单词将被一个特殊的`<UNK>`标记（表示“未知”）替换。此外，由于我们希望使用包含不同长度标题的小批量进行训练，因此我们在`<END>` 标记之后用一个特殊的`<NULL>` 标记填充短标题，并且不计算`<NULL>`标记的损失或梯度。\n",
    "\n",
    "您可以使用文件`cs231n/coco_utils.py`中的`load_coco_data` 函数加载所有COCO数据（标题、特性、url和词汇表）。运行以下单元格以执行此操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "# We'll work with dimensionality-reduced features for the remainder of this assignment,\n",
    "# but you can also experiment with the original features on your own by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "在使用数据集之前，最好先查看数据集中的示例。\n",
    "\n",
    "您可以使用文件`cs231n/coco_utils.py`中的`sample_coco_minibatch`函数从`load_coco_data`返回的数据结构中对小批量数据进行采样。运行下面的命令，对一小批训练数据进行采样，并显示图像及其标题。多次运行它并查看结果可以帮助您了解数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a minibatch and show the images and captions.\n",
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "batch_size = 3\n",
    "\n",
    "captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "    plt.imshow(image_from_url(url))\n",
    "    plt.axis('off')\n",
    "    caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "正如在讲座中所讨论的，我们将使用递归神经网络（RNN）语言模型进行图像字幕。文件`cs231n/rnn_layers.py`包含递归神经网络所需的不同层类型的实现，而文件`cs231n/classifiers/rnn.py`使用这些层来实现图像字幕模型。\n",
    "\n",
    "我们将首先在`cs231n/rnn_layers.py`中实现不同类型的RNN层。\n",
    "\n",
    "**NOTE:** 长-短期记忆（LSTM）RNN是普通RNN的一种常见变体。因为是可选的额外学分，所以现在不要担心在`cs231n/classifiers/rnn.py`和`cs231n/rnn_layers.py`中引用LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: Step Forward\n",
    "\n",
    "打开文件为`cs231n/rnn_layers.py`。该文件实现了递归神经网络中常用的不同类型层的向前和向后传递。\n",
    "\n",
    "首先实现函数`rnn_step_forward`，它实现了一个普通递归神经网络的单时间步的前向传递。执行此操作后，运行以下命令检查实现。您应该可以看到`e-8`或更少数量级的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: Step Backward\n",
    "\n",
    "在文件`cs231n/rnn_layers.py`中实现`rnn_step_backward`函数。执行此操作后，运行以下命令检查您的实现。你应该看到错误的顺序为`e-8`或更少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.rnn_layers import rnn_step_forward, rnn_step_backward\n",
    "np.random.seed(231)\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: Forward\n",
    "\n",
    "现在您已经实现了普通RNN的单个时间步的向前和向后传递，您将组合这些部分来实现处理整个数据序列的RNN。\n",
    "\n",
    "在文件`cs231n/rnn_layers.py`中，实现函数`rnn_forward`。这应该使用上面定义的`rnn_step_forward`函数来实现。执行此操作后，运行以下命令检查实现。您应该可以看到`e-7`或更少顺序的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: Backward\n",
    "\n",
    "在文件`cs231n/rnn_layers.py`中，对函数`rnn_backward`中的vanillar RNN实现向后传递。这应该在整个序列上运行反向传播，调用前面定义的`rnn_step_backward`函数。您应该看到e-6或更少的顺序上的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding: Forward\n",
    "\n",
    "在深度学习系统中，我们通常使用向量来表示单词。词汇表中的每个单词都将与一个向量相关联，这些向量将与系统的其余部分一起学习。\n",
    "\n",
    "在文件`cs231n/rnn_layers.py`中，实现函数`word_embedding_forward`将单词（由整数表示）转换为向量。运行以下命令检查您的实现。你应该看到一个错误的顺序`e-8`或更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n",
    "W = np.linspace(0, 1, num=V*D).reshape(V, D)\n",
    "\n",
    "out, _ = word_embedding_forward(x, W)\n",
    "expected_out = np.asarray([\n",
    " [[ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.42857143,  0.5,         0.57142857]],\n",
    " [[ 0.42857143,  0.5,         0.57142857],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429]]])\n",
    "\n",
    "print('out error: ', rel_error(expected_out, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding: Backward\n",
    "\n",
    "在函数`word_embedding_backward`中实现单词嵌入函数的向后传递。执行此操作后，运行以下命令检查您的实现。你应该看到一个错误的顺序为`e-11`或更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, T, V, D = 50, 3, 5, 6\n",
    "x = np.random.randint(V, size=(N, T))\n",
    "W = np.random.randn(V, D)\n",
    "\n",
    "out, cache = word_embedding_forward(x, W)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dW = word_embedding_backward(dout, cache)\n",
    "\n",
    "f = lambda W: word_embedding_forward(x, W)[0]\n",
    "dW_num = eval_numerical_gradient_array(f, W, dout)\n",
    "\n",
    "print('dW error: ', rel_error(dW, dW_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Temporal Affine Layer\n",
    "\n",
    "在每个时间步，我们使用仿射函数将该时间步的RNN隐藏向量转换为词汇表中每个单词的分数。因为这与您在赋值2中实现的仿射层非常相似，所以我们在`cs231n/rnn_layers.py`文件的`temporal_affine_forward`和`temporal_affine_backward`函数中为您提供了此函数。运行以下命令对实现执行数值渐变检查。您应该可以看到`e-9`或更少顺序的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "# Gradient check for temporal affine layer\n",
    "N, T, D, M = 2, 3, 4, 5\n",
    "x = np.random.randn(N, T, D)\n",
    "w = np.random.randn(D, M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "out, cache = temporal_affine_forward(x, w, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: temporal_affine_forward(x, w, b)[0]\n",
    "fw = lambda w: temporal_affine_forward(x, w, b)[0]\n",
    "fb = lambda b: temporal_affine_forward(x, w, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dw_num = eval_numerical_gradient_array(fw, w, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "dx, dw, db = temporal_affine_backward(dout, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Temporal Softmax Loss\n",
    "\n",
    "在RNN语言模型中，我们在每个时间步为词汇表中的每个单词生成一个分数。我们知道每个时间步的地面真值，所以我们使用softmax损失函数来计算每个时间步的损失和梯度。我们计算了一段时间内的损失，并将其平均化。\n",
    "\n",
    "但是有一个问题：由于我们对小批量进行操作，不同的标题可能有不同的长度，因此我们将`<NULL>` 标记附加到每个标题的末尾，使它们都具有相同的长度。我们不希望这些`<NULL>`标记计入损失或梯度，因此除了分数和地面真值标签之外，我们的损失函数还接受`mask`数组，告诉它分数的哪些元素计入损失。\n",
    "\n",
    "由于这与您在作业1中实现的softmax损失函数非常相似，因此我们为您实现了此损失函数；查看文件`cs231n/rnn_layers.py`中的`temporal_softmax_loss`函数。\n",
    "\n",
    "运行下面的单元格以检查丢失情况，并对函数执行数字渐变检查。您应该看到一个错误的dx的顺序为`e-7`或更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sanity check for temporal softmax loss\n",
    "from cs231n.rnn_layers import temporal_softmax_loss\n",
    "\n",
    "N, T, V = 100, 1, 10\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * np.random.randn(N, T, V)\n",
    "    y = np.random.randint(V, size=(N, T))\n",
    "    mask = np.random.rand(N, T) <= p\n",
    "    print(temporal_softmax_loss(x, y, mask)[0])\n",
    "  \n",
    "check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n",
    "check_loss(100, 10, 10, 1.0)  # Should be about 23\n",
    "check_loss(5000, 10, 10, 0.1) # Should be within 2.2-2.4\n",
    "\n",
    "# Gradient check for temporal softmax loss\n",
    "N, T, V = 7, 8, 9\n",
    "\n",
    "x = np.random.randn(N, T, V)\n",
    "y = np.random.randint(V, size=(N, T))\n",
    "mask = (np.random.rand(N, T) > 0.5)\n",
    "\n",
    "loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n",
    "\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Image Captioning\n",
    "现在您已经实现了必要的层，您可以组合它们来构建图像字幕模型。打开文件`cs231n/classifiers/rnn.py`，看看`CaptioningRNN`类。\n",
    "\n",
    "在`loss`函数中实现模型的前后传递。现在你只需要实现`cell_type='rnn'` 代表vanialla RNNs的情况；稍后将实现LSTM案例。这样做之后，使用一个小测试用例运行以下命令来检查您的前向传递；你应该看到`e-10`或更小的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, W, H = 10, 20, 30, 40\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "\n",
    "features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n",
    "captions = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "loss, grads = model.loss(features, captions)\n",
    "expected_loss = 9.83235591003\n",
    "\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的单元格对`CaptioningRNN`类执行数值渐变检查；您应该可以看到大约为`e-6`或更小的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "batch_size = 2\n",
    "timesteps = 3\n",
    "input_dim = 4\n",
    "wordvec_dim = 5\n",
    "hidden_dim = 6\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n",
    "features = np.random.randn(batch_size, input_dim)\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=input_dim,\n",
    "    wordvec_dim=wordvec_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    cell_type='rnn',\n",
    "    dtype=np.float64,\n",
    ")\n",
    "\n",
    "loss, grads = model.loss(features, captions)\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(features, captions)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s relative error: %e' % (param_name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit RNN Captioning Model on Small Data\n",
    "类似于我们在上一个任务中用来训练图像分类模型的`Solver`类，在这个任务中我们使用`CaptioningSolver`类来训练图像字幕模型。打开文件`cs231n/captioning_solver.py`，通读`CaptioningSolver`类；看上去应该很眼熟。\n",
    "\n",
    "一旦您熟悉了API，请运行以下命令，以确保您的模型适合100个训练示例的小样本。最终损失应小于0.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "small_data = load_coco_data(max_train=50)\n",
    "\n",
    "small_rnn_model = CaptioningRNN(\n",
    "    cell_type='rnn',\n",
    "    word_to_idx=data['word_to_idx'],\n",
    "    input_dim=data['train_features'].shape[1],\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    ")\n",
    "\n",
    "small_rnn_solver = CaptioningSolver(\n",
    "    small_rnn_model, small_data,\n",
    "    update_rule='adam',\n",
    "    num_epochs=50,\n",
    "    batch_size=25,\n",
    "    optim_config={\n",
    "     'learning_rate': 5e-3,\n",
    "    },\n",
    "    lr_decay=0.95,\n",
    "    verbose=True, print_every=10,\n",
    ")\n",
    "\n",
    "small_rnn_solver.train()\n",
    "\n",
    "# Plot the training losses.\n",
    "plt.plot(small_rnn_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print final training loss. You should see a final loss of less than 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnn_final_training_loss"
   },
   "outputs": [],
   "source": [
    "print('Final loss: ', small_rnn_solver.loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Sampling at Test Time\n",
    "与分类模型不同，图像字幕模型在训练时和测试时的表现非常不同。在训练时，我们可以访问地面实况字幕，因此我们在每个时间步将地面实况词作为输入输入输入到RNN。在测试时，我们在每个时间步从词汇表的分布中采样，并在下一个时间步将样本作为输入提供给RNN。\n",
    "\n",
    "在文件`cs231n/classifiers/rnn.py`中，实现测试时间采样的`sample`方法。完成此操作后，运行以下命令从过度拟合的模型中对训练和验证数据进行采样。训练数据的样本应该很好。然而，验证数据上的样本可能没有意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "for split in ['train', 'val']:\n",
    "    minibatch = sample_coco_minibatch(small_data, split=split, batch_size=2)\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "    sample_captions = small_rnn_model.sample(features)\n",
    "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "        img = image_from_url(url)\n",
    "        # Skip missing URLs.\n",
    "        if img is None: continue\n",
    "        plt.imshow(img)          \n",
    "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "# Inline Question 1\n",
    "\n",
    "在我们当前的图像字幕设置中，我们的RNN语言模型在每个时间步产生一个单词作为它的输出。然而，提出问题的另一种方法是训练网络在字符（例如“a”、“b”等）上操作，而不是在单词上操作，这样在每一个时间步，它都会接收前一个字符作为输入，并尝试预测序列中的下一个字符。例如，网络可能会生成一个标题，如\n",
    "\n",
    "'A', ' ', 'c', 'a', 't', ' ', 'o', 'n', ' ', 'a', ' ', 'b', 'e', 'd'\n",
    "\n",
    "你能描述一下使用字符级RNN的图像字幕模型的一个优点吗？你能描述一下一个缺点吗？提示：有几个有效的答案，但是比较单词级和字符级模型的参数空间可能会有用。\n",
    "\n",
    "**Your Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Learning",
   "language": "python",
   "name": "wy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
